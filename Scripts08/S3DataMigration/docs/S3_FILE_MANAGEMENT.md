# üóëÔ∏è **S3 File Management Strategies**

## üéØ **Default Behavior: OVERWRITE = TRUE**

**Current implementation:** Files are **overwritten** on subsequent runs to the same location.

---

## üìã **Behavior Explained**

### **Scenario 1: Same Path, OVERWRITE = TRUE (Default)**

#### **First Run:**
```sql
COPY INTO 's3://cm-migration-dev01/ANALYTICS/BI/DIMPAYER/run_20251221_100000/'
FROM (SELECT * FROM ANALYTICS.BI.DIMPAYER)
OVERWRITE = TRUE;
```

**Result:**
```
s3://cm-migration-dev01/ANALYTICS/BI/DIMPAYER/run_20251221_100000/
‚îî‚îÄ‚îÄ data_0_0_0.snappy.parquet  (160 rows, created at 10:00 AM)
```

#### **Second Run (Same Path):**
```sql
-- Same path again
COPY INTO 's3://cm-migration-dev01/ANALYTICS/BI/DIMPAYER/run_20251221_100000/'
FROM (SELECT * FROM ANALYTICS.BI.DIMPAYER)
OVERWRITE = TRUE;
```

**Result:**
```
s3://cm-migration-dev01/ANALYTICS/BI/DIMPAYER/run_20251221_100000/
‚îî‚îÄ‚îÄ data_0_0_0.snappy.parquet  (160 rows, REPLACED at 11:00 AM)
```

**‚úÖ Old file deleted, new file created**

---

### **Scenario 2: Same Path, OVERWRITE = FALSE**

If you set `overwrite: false` in config:

#### **First Run:**
```sql
COPY INTO 's3://cm-migration-dev01/ANALYTICS/BI/DIMPAYER/run_20251221_100000/'
FROM (SELECT * FROM ANALYTICS.BI.DIMPAYER)
OVERWRITE = FALSE;
```

**Result:**
```
s3://cm-migration-dev01/ANALYTICS/BI/DIMPAYER/run_20251221_100000/
‚îî‚îÄ‚îÄ data_0_0_0.snappy.parquet  (created)
```

#### **Second Run (Same Path):**
```sql
COPY INTO 's3://cm-migration-dev01/ANALYTICS/BI/DIMPAYER/run_20251221_100000/'
FROM (SELECT * FROM ANALYTICS.BI.DIMPAYER)
OVERWRITE = FALSE;
```

**Result:**
```
‚ùå ERROR: Files already exist in the path
```

**Snowflake will fail with error:** `Files already exist and OVERWRITE is set to FALSE`

---

### **Scenario 3: Different Path Each Run (Recommended)**

Use unique run_id for each migration:

#### **Run 1:**
```python
s3_path = "s3://cm-migration-dev01/ANALYTICS/BI/DIMPAYER/run_20251221_100000/"
```

#### **Run 2:**
```python
s3_path = "s3://cm-migration-dev01/ANALYTICS/BI/DIMPAYER/run_20251221_110000/"
```

**Result:**
```
s3://cm-migration-dev01/ANALYTICS/BI/DIMPAYER/
‚îú‚îÄ‚îÄ run_20251221_100000/
‚îÇ   ‚îî‚îÄ‚îÄ data_0_0_0.snappy.parquet  (kept)
‚îî‚îÄ‚îÄ run_20251221_110000/
    ‚îî‚îÄ‚îÄ data_0_0_0.snappy.parquet  (new)
```

**‚úÖ Both runs preserved, no conflicts**

---

## üõ†Ô∏è **Configuration Options**

### **Option 1: Always Overwrite (Current Default)**

**Best for:**
- Testing and development
- When you only need the latest data
- When storage cost is a concern

**Configuration:**
```json
{
  "snowflake_unload": {
    "overwrite": true
  },
  "s3_staging": {
    "cleanup_after_load": false
  }
}
```

**Behavior:**
- ‚úÖ Second run to same path overwrites files
- ‚úÖ No error on re-run
- ‚úÖ Lower S3 storage costs

---

### **Option 2: Never Overwrite (Strict)**

**Best for:**
- Production with audit requirements
- When you need history of all runs
- When errors should be explicit

**Configuration:**
```json
{
  "snowflake_unload": {
    "overwrite": false
  }
}
```

**Behavior:**
- ‚ùå Second run to same path will ERROR
- ‚úÖ Forces you to use unique paths (run_id)
- ‚úÖ Complete history preserved

---

### **Option 3: Unique Paths + Cleanup (Recommended)**

**Best for:**
- Production migrations
- Balance between history and storage
- Clean S3 bucket

**Configuration:**
```json
{
  "snowflake_unload": {
    "overwrite": true,
    "include_query_id": true
  },
  "s3_staging": {
    "cleanup_after_load": true
  }
}
```

**Behavior:**
1. ‚úÖ Each run uses unique path (timestamp-based run_id)
2. ‚úÖ Files loaded into PostgreSQL
3. ‚úÖ Python deletes S3 files after successful load
4. ‚úÖ Keeps S3 clean, saves costs

---

## üîß **How to Change Behavior**

### **Method 1: Update Config File**

Edit `s3copyconfig.json`:

```json
{
  "snowflake_unload": {
    "overwrite": true,  ‚Üê Change to false
    "include_query_id": true
  },
  "s3_staging": {
    "cleanup_after_load": false  ‚Üê Change to true for auto-cleanup
  }
}
```

---

### **Method 2: Override in Code**

When calling the unload function:

```python
# In test_snowflake_unload.py or your script
result = unloader.unload_table(
    source_database="ANALYTICS",
    source_schema="BI",
    source_table="DIMPAYER",
    s3_path=s3_path,
    overwrite=False  ‚Üê Override here
)
```

---

## üéØ **Recommended Strategy by Use Case**

### **For Testing (Phase 1-3, Current):**

```json
{
  "snowflake_unload": {
    "overwrite": true  ‚Üê Allow re-runs without errors
  }
}
```

**Why:**
- You'll re-run tests multiple times
- Don't want to create new folders each time
- Easy to iterate

**Path pattern:**
```
ANALYTICS/BI/DIMPAYER/test_100_rows/  ‚Üê Fixed path, overwrite each test
```

---

### **For Development (Iterating on Full DIMPAYER):**

```json
{
  "snowflake_unload": {
    "overwrite": true
  }
}
```

**Path pattern:**
```python
# Use fixed run_id for development
s3_path = "s3://cm-migration-dev01/ANALYTICS/BI/DIMPAYER/dev_latest/"
```

**Benefit:** Always overwrite `dev_latest/` folder during development

---

### **For Production (Phase 8, Final):**

```json
{
  "snowflake_unload": {
    "overwrite": true,
    "include_query_id": true
  },
  "s3_staging": {
    "cleanup_after_load": true
  }
}
```

**Path pattern:**
```python
# Unique run_id each time
run_id = datetime.now().strftime('%Y%m%d_%H%M%S')
s3_path = f"s3://cm-migration-dev01/ANALYTICS/BI/DIMPAYER/run_{run_id}/"
```

**Workflow:**
1. Run 1: UNLOAD ‚Üí `run_20251221_100000/` ‚Üí Load to PG ‚Üí Delete from S3
2. Run 2: UNLOAD ‚Üí `run_20251221_110000/` ‚Üí Load to PG ‚Üí Delete from S3
3. S3 stays clean, each run has unique path

---

## üóëÔ∏è **Cleanup Strategies**

### **Manual Cleanup (Current)**

```bash
# Delete specific run
aws s3 rm s3://cm-migration-dev01/ANALYTICS/BI/DIMPAYER/run_20251221_100000/ --recursive

# Delete all DIMPAYER data
aws s3 rm s3://cm-migration-dev01/ANALYTICS/BI/DIMPAYER/ --recursive

# Delete old test runs
aws s3 rm s3://cm-migration-dev01/ANALYTICS/BI/DIMPAYER/test_100_rows/ --recursive
```

---

### **Automated Cleanup (Future Phase 4-6)**

When PostgreSQL load is implemented:

```python
# After successful load to PostgreSQL
if load_successful and config.get('cleanup_after_load'):
    s3_manager.delete_files(s3_keys_loaded)
    logger.info("‚úÖ Cleaned up S3 files after successful load")
```

---

### **S3 Lifecycle Policy (Production)**

Set bucket lifecycle rule in AWS:

```json
{
  "Rules": [
    {
      "Id": "DeleteOldMigrationFiles",
      "Status": "Enabled",
      "Prefix": "ANALYTICS/BI/",
      "Expiration": {
        "Days": 7
      }
    }
  ]
}
```

**Benefit:** Auto-delete files older than 7 days

---

## üìä **Behavior Summary Table**

| Scenario | OVERWRITE | run_id | Behavior | Use Case |
|----------|-----------|--------|----------|----------|
| **Test Runs** | TRUE | Fixed (`test_100_rows`) | Overwrites each time | Testing |
| **Dev Runs** | TRUE | Fixed (`dev_latest`) | Overwrites each time | Development |
| **Prod (No Cleanup)** | TRUE | Unique (timestamp) | New folder each run | Audit trail |
| **Prod (With Cleanup)** | TRUE | Unique (timestamp) | New folder + auto-delete | Clean S3 |
| **Strict Mode** | FALSE | Unique (timestamp) | Error if exists | High security |

---

## üéØ **Recommended: Current Setup**

For your testing phase, the **current default is perfect**:

```json
{
  "snowflake_unload": {
    "overwrite": true
  }
}
```

**Why:**
- ‚úÖ Test runs won't error on re-run
- ‚úÖ Can iterate quickly
- ‚úÖ Fixed paths like `test_100_rows/` get overwritten
- ‚úÖ Easy to verify results (same location each time)

---

## üöÄ **When You Deploy to Production**

Switch to unique paths + cleanup:

```json
{
  "snowflake_unload": {
    "overwrite": true,
    "include_query_id": true
  },
  "s3_staging": {
    "cleanup_after_load": true
  }
}
```

Plus add S3 lifecycle policy for safety net.

---

## ‚úÖ **Quick Answers**

**Q: Are files deleted on second run?**  
**A:** With default `overwrite: true` - **YES** ‚úÖ

**Q: Can I keep multiple runs?**  
**A:** **YES** - Use unique run_id for each run ‚úÖ

**Q: Can I prevent overwrite?**  
**A:** **YES** - Set `overwrite: false` in config ‚úÖ

**Q: Can I auto-cleanup after load?**  
**A:** **YES** - Set `cleanup_after_load: true` (Phase 4+) ‚úÖ

---

## üìù **For Your Current Testing**

**Keep the defaults!** Your current config is perfect for testing:

```json
{
  "snowflake_unload": {
    "overwrite": true  ‚Üê Perfect for testing
  }
}
```

**This means:**
- ‚úÖ Run `test_snowflake_unload.py --table DIMPAYER --rows 100` multiple times
- ‚úÖ Each run overwrites `DIMPAYER/test_100_rows/` folder
- ‚úÖ No errors, clean iteration
- ‚úÖ Easy to verify same location each time

**Later for production:** Switch to unique paths + cleanup strategy!

