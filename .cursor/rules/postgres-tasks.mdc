---
description: Conflict detection v3 architecture, execution flow, and ECS container implementation (Conflict/postgres)
globs: Conflict/postgres/**/*.py, Conflict/postgres/**/*.sql, Conflict/postgres/**/*.json, Scripts13/**/*.py, Scripts13/**/*.sql, Scripts13/**/*.json
alwaysApply: false
---

# Conflict Detection Pipeline -- ECS Container (Conflict/postgres)

## Overview

PostgreSQL conflict detection pipeline deployed as a Docker container on AWS ECS/ECR. Detects scheduling conflicts by comparing Snowflake visit data against PostgreSQL conflict tables, with streaming batch processing, conditional flag updates, and pair-precise stale cleanup.

## Databases

- **Snowflake (read-only source)**: `ANALYTICS.BI.FACTVISITCALLPERFORMANCE_CR` view with dimension joins. RSA key auth, streaming cursor. Temp tables require `database` and `schema` on connection.
- **PostgreSQL (read/write target)**: `conflict_management.conflict_dev` schema. Tables: `conflicts` (parent), `conflictvisitmaps` (detail with 7 conflict flags). psycopg2 driver. All column names double-quoted (case-sensitive).

## Repository Structure (Conflict/postgres/)

```
Conflict/postgres/
├── scripts/
│   ├── main.py                          # ECS entry point, ACTION dispatching, SIGTERM handling
│   └── actions/
│       ├── task00_preflight.py           # Pre-run validation, pg_cron disable, InProgressFlag, sequence sync
│       ├── task01_copy_to_staging.py     # PPR sync from Snowflake dims, staging populate
│       ├── task02_00_conflict_update.py  # v3 conflict detection UPDATE + INSERT pipeline
│       ├── task02_01_inservice_conflict.py # InService conflict detection (update + insert)
│       ├── task03_status_management.py    # Status cascade, deleted visits, computed columns
│       ├── task99_postflight.py          # VACUUM/ANALYZE, pg_cron re-enable, email, summary
│       ├── validate_config.py           # Standalone config validation
│       └── test_connections.py          # Standalone connectivity test
├── lib/
│   ├── conflict_processor.py            # Core: v3 streaming, batch updates, stale cleanup
│   ├── query_builder.py                 # Builds Snowflake SQL from v3 templates
│   ├── connections.py                   # Snowflake and PostgreSQL connection managers
│   ├── utils.py                         # Logging, formatting, memory estimation
│   └── email_sender.py                  # AWS SES HTML email sender
├── sql/
│   ├── sf_task01_dim_payer_provider.sql  # Task 01: Snowflake payer-provider dimension query
│   ├── sf_task02_00_step1_delta_keys.sql
│   ├── sf_task02_00_step2_base_visits.sql
│   ├── sf_task02_00_step3_final_query.sql
│   ├── sf_task02_01_step1_visits.sql     # InService: eligible visits temp table
│   ├── sf_task02_01_step2_events.sql     # InService: events with synthetic MD5 VisitID
│   ├── sf_task02_01_step3_pairs.sql      # InService: UNION ALL both directions
│   ├── pg_fetch_excluded_ssns.sql
│   ├── pg_fetch_excluded_agencies.sql
│   ├── pg_fetch_mph.sql
│   └── pg_fetch_settings.sql
├── config/
│   ├── config.json                      # Runtime config (lookback_hours, batch_size, pipeline, email)
│   └── settings.py                      # Config loader with JSON-safe env var substitution
├── deploy/
│   ├── build-and-push-ecr.ps1           # Interactive deploy script (SSO, build, push, register, run)
│   ├── ecs-task-definition.json         # ECS Fargate task definition template
│   ├── .env.example                     # Template for deploy/.env
│   ├── .env                             # Secrets (gitignored)
│   └── .generated-taskdef.json          # Resolved output (gitignored)
├── tests/
│   ├── test_comprehensive.py            # 204 pytest tests
│   └── sql/                             # Standalone test queries for manual Snowflake testing
├── docs/
│   ├── README.md
│   ├── ECS_DEPLOYMENT_GUIDE.md
│   ├── TESTING_GUIDE.md
│   └── TROUBLESHOOTING.md
├── Dockerfile
├── requirements.txt
├── .gitignore
└── .dockerignore
```

## v3 Execution Flow

1. **Step 0**: Load excluded SSNs into Snowflake temp table (`excluded_ssns_temp`)
2. **Step 1**: Create `delta_keys` temp table (DISTINCT VisitDate, SSN from recent updates)
3. **Step 2 Part A**: Create `base_visits` with delta rows only (`is_delta=1`)
4. **Step 2 Part B** (asymmetric): INSERT related non-delta rows via `INNER JOIN delta_keys` (`is_delta=0`)
5. **Step 2d**: Stream `(visit_date, ssn)` pairs from `delta_keys` to PostgreSQL `_tmp_delta_pairs` via chunked COPY
6. **Step 3**: Self-join `base_visits` with `V1.is_delta=1` constraint (asymmetric), apply cross-state filter (CTEs 7-9), stream conflicts
7. **Step 4**: Pair-precise stale cleanup -- Phase 1: identify stale via anti-join on `_tmp_delta_pairs`; Phase 2: batched UPDATE (100K chunks), `StatusFlag='R'`

## Critical Design Decisions

- **Two-part base_visits (CREATE + INSERT, not UNION ALL)**: Same SQL template formatted twice with different placeholders. Two separate statements give per-leg timing in logs and avoid the `OR + IN` subquery that prevented Snowflake partition pruning. Both parts share 120 columns + 11 dimension JOINs -- duplication is unavoidable since Step 3 needs resolved columns.
- **`is_delta` flag**: Constrains self-join to Delta-vs-All (not All-vs-All on 9.6M rows)
- **Streaming cursor + batch processing**: Step 3 results streamed from Snowflake (not fetched all at once). Batches of 5,000 rows: fetch existing PG records, detect changes (7 flags + 40 columns), UPDATE dirty rows only, commit per batch. Single-threaded by design (bottleneck is Snowflake query time, not Python processing).
- **Pair-precise stale cleanup**: Uses exact `(VisitDate, SSN)` pairs streamed via chunked COPY (100K/chunk), NOT separate DISTINCT lists (cross-product caused 3.9M false stale positives)
- **UUID type matching**: `_tmp_seen_conflicts` uses UUID columns to match `conflictvisitmaps` indexes
- **Conflict flag simplification**: 7 rules omit redundant `ProviderID != ConProviderID` (enforced by JOIN). Direct column `=` comparison instead of CONCAT-based string ops.
- **StatusFlag values**: 'N'=New/Active, 'U'=Updated (set on re-detection), 'R'=Resolved/Stale, 'W'=Whitelisted, 'I'=Ignored (W and I never overwritten)

## Stats Fields (backward compat aliases)

- `delta_keys_count` -> alias for `delta_pairs_count`
- `modified_visit_ids_count` -> alias for `delta_ssns_count`
- `stale_conflicts_reset` -> alias for `stale_conflicts_resolved`

## Code Cleanup Notes

- v2 methods removed: `stream_and_process_conflicts()`, `build_conflict_detection_query()`, `_build_symmetric_conflict_pairs()`, `_build_asymmetric_conflict_pairs()`, `execute_batch_update()`, `fetch_reference_data()`
- v1/v2 SQL template and default files deleted (no longer referenced by any code)
- `delta_keys` (step1) is always created in both modes (needed for stale cleanup scoping, not just asymmetric join)

## Latest Performance Runs (2026-02-13, asymmetric + stale + insert + cross-state filter)

### 36h lookback (baseline validation with cross-state filter)
| Metric | Value |
|---|---|
| Total pipeline time | 12m 28s |
| task02 time | 6m 59s (419s) |
| Rows fetched from Snowflake | 99,883 |
| Matched in Postgres | 99,883 (100%) |
| New conflicts | 0 (all previously inserted) |
| Updates needed | 0 (no data changes) |
| Stale conflicts resolved | 49 → StatusFlag='R' (cross-state pairs) |
| Delta pairs (stale scope) | 8,832,316 (644K SSNs × 596 dates) |

### 60h lookback (stress test)
| Metric | Value |
|---|---|
| Total pipeline time | 12m 42s |
| task02 time | 8m 17s (498s) |
| Rows fetched from Snowflake | 152,073 |
| Matched in Postgres | 114,577 (75%) |
| New conflicts inserted | 37,496 |
| Rows updated | 79 |
| Stale conflicts resolved | 0 |
| Delta pairs (stale scope) | 12,745,668 (646K SSNs × 596 dates) |
| Errors | 0 |

### Time breakdown (60h run):
- Step 0 (excluded SSNs): ~7s
- Step 1 (delta_keys): ~9s
- Step 2 (base_visits A+B): ~145s
- Step 2d (delta pairs → PG): ~92s
- Step 3 (conflict detection + streaming + inserts): ~197s
- Step 4 (stale cleanup): ~41s

### InService task02_01 (with caregiver semi-join optimisation)
| Metric | Value |
|---|---|
| Total pipeline time | 16m 2s (with preflight + postflight) |
| task02_01 time | 14m 16s (857s) |
| Snowflake Step 1 (visits temp) | 233s |
| Snowflake Step 2 (events temp) | 9.6s |
| Snowflake Step 3 (pairs + processing) | 600s |
| Rows fetched from Snowflake | 165,826 |
| Unique VisitIDs | 148,681 |
| Rows updated | 165,826 |
| Rows inserted | 0 (steady state, all pairs already exist) |
| Errors | 0 |

Note: Caregiver semi-join in Step 1 reduced rows by 28% vs initial run (230K → 166K) and cut task02_01 time from 21min to 14min.

## Current State (as of 2026-02-13)

### What's working:
- v3 multi-step Snowflake query architecture (delta_keys -> base_visits -> self-join)
- Asymmetric join with is_delta flag optimization
- Streaming cursor + 5K-row batch processing with commit-per-batch
- Conditional flag updates (7 flags: only N->Y, StatusFlag W/I preserved)
- **INSERT of new conflicts** integrated into streaming pass (~102K/run, StatusFlag='N', 206-column mapping, `enable_insert` config flag, self-healing PK retry)
- **InService conflict detection** (`task02_01_inservice_conflict`): self-contained action for visits vs InService events at different providers. 3-step Snowflake queries (visits temp → events temp → UNION ALL pairs), temporal overlap join, synthetic MD5 VisitIDs, 7 flags hardcoded 'N', `InServiceFlag='Y'`, UUID key normalisation for PG/Snowflake matching, row-by-row insert fallback for UniqueViolation safety. Caregiver semi-join optimisation in Step 1 pre-filters to only caregivers with InService events (~28% row reduction). ~166K rows processed, ~14 min per run.
  - **Future optimisation (DEFERRED)**: Delta approach using `FACTCAREGIVERINSERVICE."Updated Date"` + `FACTVISITCALLPERFORMANCE_CR."Visit Updated Timestamp"` with asymmetric UNION (delta_visits × ALL events UNION ALL visits × delta_events). Would reduce 166K full-scan to only recently-changed pairs (~10-20K), cutting runtime to ~3-5 min. Deferred because 14 min is acceptable, InService data changes infrequently, and the complexity/risk outweighs the gain at current volumes.
- Pair-precise stale cleanup (12.5M pairs, seen-based anti-join, StatusFlag='R')
- Change detection (_has_changes: 7 flags + 40 business columns)
- Excluded SSNs loaded from PostgreSQL into Snowflake temp table
- Column name mapping (ETATravleMinutes->ETATravelMinutes, SchVisitTimeSame->SchAndVisitTimeSameFlag)
- 204 pytest tests covering all of the above
- **ECS containerization COMPLETE**: Dockerfile, task definition, ECR deployment, interactive deploy script
- Entry point: `scripts/main.py` with comma-separated ACTION env var and DEFAULT_ACTIONS pipeline
- JSON-safe env var substitution in `settings.py` (handles RSA keys with newlines)
- Graceful SIGTERM shutdown handling
- **Modular action architecture**: main.py is a lean orchestrator; each action in `scripts/actions/<key>.py`
- **task01_copy_to_staging COMPLETE**: PPR sync via temp-table + UPDATE FROM JOIN (~3s), staging populate (~280s for 8M rows)
- **Pipeline summary logging**: Plain-text summary always logged to CloudWatch (independent of email)
- **Cross-state conflict filter** in Step 3 SQL (CTEs 7-9: `state_map` + `cross_state_prep` + `same_state_conflicts`): excludes conflicts where no address state on V1 side matches any on V2 side. State normalization via `state_map` CTE with LEFT JOIN lookups (single definition, no repetition), provider address fallback when both patient addresses NULL, matching `SP_CLEANUP_CROSS_STATE_CONFLICTS` logic exactly. Prevents both INSERT and UPDATE of cross-state pairs; stale cleanup auto-resolves existing ones.
- **Identity sequence sync** in preflight step 6: auto-advances identity sequences to MAX(ID) if behind, with self-healing retry in `_execute_inserts_with_commit`
- Successfully deployed and tested on ECS Fargate (~14 min full pipeline with task01)

### What's NOT yet done:
- AWS Secrets Manager integration (currently using plain environment variables)

## ECS Architecture Notes

### Container Entry Point (`scripts/main.py`)
- `ACTION=""` (default): runs full pipeline `task00_preflight -> task01_copy_to_staging -> task02_00_conflict_update -> task02_01_inservice_conflict -> task03_status_management -> task99_postflight`
- `ACTION="test_connections"`: runs single action
- `ACTION="validate_config,test_connections"`: comma-separated sequential execution
- SIGTERM handler for graceful ECS shutdown (30s stop timeout)
- All parameters from env vars with config.json defaults as fallback
- **Postflight runs even on failure** (re-enables pg_cron, clears InProgressFlag)

### Action Naming Convention
- File: `<registry_key>.py` (e.g. `task01_copy_to_staging.py`)
- Function: `run_<registry_key>()` (e.g. `run_task01_copy_to_staging()`)
- Import: `from scripts.actions.<key> import run_<key>`

### Pipeline Actions (DEFAULT_ACTIONS)
- `task00_preflight`: 1) Validate config params, 2) Verify Snowflake+PG connectivity, 3) Check required tables, 4) Disable pg_cron job, 5) Set InProgressFlag=1, 6) Sync identity sequences (advance to MAX(ID) if behind), 7) VACUUM all tables, 8) ANALYZE all tables, 9) Capture pre-run row counts
- `task01_copy_to_staging`: 1) Sync payer_provider_reminders from Snowflake dims (temp-table + UPDATE FROM JOIN), 2) Truncate+populate conflictlog_staging from conflictvisitmaps+conflicts (date-filtered)
- `task02_00_conflict_update`: v3 conflict detection pipeline (streaming cursor, batch UPDATE + INSERT, stale cleanup)
- `task02_01_inservice_conflict`: InService conflict detection -- visits vs InService events at different providers. Temporal overlap join, synthetic MD5 VisitIDs, 7 flags hardcoded 'N', UUID key normalisation (_norm_key), row-by-row insert fallback. ~230K rows, ~10K inserts, ~20min.
- `task03_status_management`: Post-conflict-creation status management (15 steps: 14 SQL + 1 fetch, 3 phases, ~10 min steady state). Phase B (disabled by default, `enable_phase_b` config): delta query on `FACTVISITCALLPERFORMANCE_DELETED_CR` using `"Visit Updated Timestamp"` + `lookback_hours` (~97K rows vs 25.5M full scan), indexed PG temp table, UNION CTE marks CVM StatusFlag='D' when either VisitID or ConVisitID is deleted, cascade to CF. Phase A (9 steps): IsMissed cascade, UpdateFlag cleanup, conflicts aggregation, CTE-driven combined cascades, NoResponseFlag reset (forced seq scan) (~7 min). Phase C (3 steps): ShVTSTTime/CShVTSTTime COALESCE, BilledRateMinute (::real cast + epsilon comparison), ReverseUUID (~2 min). Each step commits independently. Supports `only_steps` config/env var for targeted step execution.
- `task99_postflight`: 1) VACUUM all tables, 2) ANALYZE all tables, 3) Set InProgressFlag=0, 4) Refresh materialized view (CONCURRENTLY), 5) Re-enable pg_cron job, 6) Capture post-run row counts, 7) Log pipeline summary, 8) Send SES email (if enabled)

### Standalone Actions (not in default pipeline, available via ACTION env var)
- `validate_config`: Print and validate config summary (included in preflight)
- `test_connections`: Test Snowflake and PostgreSQL connectivity (included in preflight)

### Key Design: Preflight/Postflight
- Config validation runs first (before any DB connections) to fail fast
- pg_cron commands connect to `postgres` database (not `conflict_management`)
- VACUUM/ANALYZE require `autocommit=True` on psycopg2 connections
- VACUUM/ANALYZE and InProgressFlag=0 run before MV refresh and pg_cron re-enable (cleanup first, then restore)
- Postflight runs even on pipeline failure (re-enables pg_cron, clears InProgressFlag)
- Postflight receives all prior action results via `_pipeline_results` module-level list
- Email via `boto3` SES with HTML template matching Snowflake SEND_TASK_STATUS_EMAIL format
- `config.json` has `pipeline` section (pg_cron job name, materialized view, required tables) and `email` section (enabled, region, sender, recipients)

### Deployment
- `deploy/build-and-push-ecr.ps1`: Interactive PowerShell script with 5 steps (each has a default, Enter to accept):
  1. SSO login [default: No]
  2. Docker build [default: Yes]
  3. ECR push [default: Yes if built, No if skipped]
  4. Register task definition [default: No -- only needed when env vars or CPU/memory change]
  5. ECS run-task (action selection menu, default: Skip)
- `deploy/ecs-task-definition.json`: Template with `<ACCOUNT_ID>`, `<REGION>`, `<CPU>`, `<MEMORY>`, `<YOUR_*>` placeholders
- CPU/memory configurable via `$Cpu` / `$Memory` in the script's CONFIGURATION section
- `deploy/.env`: Secret values for template resolution (gitignored). Copy from `deploy/.env.example`.
- `deploy/.env.example`: Template .env with placeholder values for onboarding
- `deploy/.generated-taskdef.json`: Resolved task definition output (gitignored, written by script)
- CloudWatch log group `/ecs/task02-conflict-updater` must be created manually before first run

## Conventions

- Python 3.11 runtime
- SQL templates loaded from `sql/` dirs, parameterized with `.format()`
- Config via `config/config.json` with `${ENV_VAR}` placeholders

## Migration Note

This project originated in `Scripts13/tasks/` within the `postgres-migration` mono-repo (which itself evolved from `Scripts12/` Lambda implementation). The code is fully self-contained -- all internal imports use relative paths (`lib/`, `scripts/`, `sql/`, `config/`). When moving to `Conflict/postgres/` in a new repo, copy the folder contents and this `.cursor/rules/postgres-tasks.mdc` file, then remove the `Scripts13/**` glob patterns from the frontmatter.
