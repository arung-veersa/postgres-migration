# Historical Issues & Resolutions

This document tracks significant bugs that were diagnosed and fixed. Useful reference for troubleshooting similar issues.

---

## Issue #1: Duplicate CloudWatch Logs (Fixed v2.3, Dec 2025)

### Problem
Every log entry appeared twice in CloudWatch Logs with slightly different formatting:
```
2025-12-15 16:48:19 | INFO     | lambda_handler | Lambda invoked...
2025-12-15 16:48:19 | INFO | lambda_handler | Lambda invoked...
```

### Root Cause
- AWS Lambda runtime automatically adds a logging handler
- Our `lib/utils.py` code added another handler
- Both handlers output the same log message
- Even with `logger.propagate = False`, both handlers activated

### Fix Applied
**File:** `lib/utils.py` - `get_logger()` function

Added Lambda environment detection:
```python
# Check if we're in AWS Lambda
in_lambda = 'AWS_EXECUTION_ENV' in os.environ or 'AWS_LAMBDA_FUNCTION_NAME' in os.environ

if in_lambda:
    # In Lambda: Use Lambda's existing handler, just set level
    logger.setLevel(logging.INFO)
    # Don't add new handlers - Lambda already has one
else:
    # Local: Add our own handler
    handler = logging.StreamHandler(sys.stdout)
    logger.addHandler(handler)
```

### Impact
- âœ… Eliminated duplicate logs (50% CloudWatch volume reduction)
- âœ… 50% cost reduction for CloudWatch Logs
- âœ… Clearer logs for debugging
- âœ… Same code works in both Lambda and local environments

---

## Issue #2: Resume Window Too Short (Fixed v2.3.1, Dec 2025)

### Problem
Migration running for > 12 hours would:
- Create a NEW `run_id` unexpectedly
- Start from scratch despite partial data in table
- Lose all progress from previous run
- Logs show "NO RESUMABLE RUN FOUND" with same config_hash

**Example from analytics_dev migration:**
```
Run 1 (cc20f58d): Started Dec 15, 4:48 PM, ran for 12 hours
Run 2 (8e37281d): Created Dec 16, 4:52 AM (12h 4min later)
  âŒ Lost all progress from Run 1
  âŒ Re-created chunks (10,770 â†’ 10,976)
  âŒ Hit duplicate key errors on existing data
```

### Root Cause
- Default `resume_max_age` was 12 hours
- Large migrations (272M rows) take days, not hours
- After 12 hours, resume detection filters out the old run
- Creates new run_id even though old run still valid
- Truncation protection prevents data loss but causes duplicates

**Code location:** 
```python
# lambda_handler.py, migration_orchestrator.py, migrate.py
resume_max_age = defaults.get('resume_max_age', 12)  # Too short!
```

### Fix Applied
**Files Modified:**
- `scripts/lambda_handler.py` - Default: 12 â†’ 168
- `scripts/migration_orchestrator.py` - Default: 12 â†’ 168  
- `migrate.py` - Default: 12 â†’ 168
- `aws/step_functions/*.json` - Step Function default: 12 â†’ 168

**New Default:** 168 hours (7 days)

**Runtime Override Available:**
```json
{
  "source_name": "analytics",
  "resume_max_age": 8760
}
```

### Why 7 Days?
- Most large migrations complete within 7 days
- Provides safety (won't resume year-old abandoned runs)
- Can be extended to 1 year (8760h) if needed
- Balance between operational hygiene and practicality

### Impact
- âœ… Prevents unexpected new run_id creation
- âœ… Large migrations can run for days/weeks without issues
- âœ… No more progress loss for long-running operations
- âœ… Still provides safety against resuming very old runs
- âœ… Configurable per-execution if needed

### Detection Query
```sql
-- Check if your run is close to expiring
SELECT 
    run_id,
    metadata->>'source_names' as sources,
    status,
    started_at,
    NOW() - started_at as age,
    EXTRACT(EPOCH FROM (NOW() - started_at))/3600 as hours_old,
    CASE 
        WHEN EXTRACT(EPOCH FROM (NOW() - started_at))/3600 < 168 
        THEN 'âœ… Safe'
        ELSE 'âš ï¸  Near expiration'
    END as status
FROM migration_status.migration_runs
WHERE status = 'running'
ORDER BY started_at DESC;
```

### Emergency Fix (If Already Expired)
```sql
-- Reset clock if > 7 days old
UPDATE migration_status.migration_runs
SET started_at = NOW()
WHERE status = 'running'
  AND NOW() - started_at > INTERVAL '7 days'
  AND metadata->>'source_names' LIKE '%analytics%'
RETURNING run_id, 'Clock reset' as message;
```

---

## Issue #3: Chunking Slowness (Fixed v2.2/2.3, Dec 2025)

### Problem
Date-based chunking taking 11 minutes before migration could start.

### Root Cause
`DateRangeStrategy` was making 199 individual COUNT queries to Snowflake:
```python
# Original code (BAD):
for date in date_range:
    query = f"SELECT COUNT(*) FROM table WHERE DATE(column) = '{date}'"
    count = snowflake.execute(query)  # 3-4 seconds per query
# Total: 199 queries Ã— 4 seconds = 11 minutes
```

### Fix Applied
**File:** `lib/chunking.py` - `DateRangeStrategy.create_chunks()`

Single aggregated query:
```python
# New code (GOOD):
aggregated_query = f"""
    SELECT 
        DATE({date_column}) as date_value,
        COUNT(*) as row_count
    FROM {schema}.{table}
    GROUP BY DATE({date_column})
    ORDER BY date_value
"""
date_counts = self.snowflake_manager.execute_query(aggregated_query)
# Total: 1 query = 0.3 seconds
```

### Impact
- âœ… Startup time: 11 minutes â†’ 0.3 seconds (2,200x speedup)
- âœ… Reduced Snowflake compute costs
- âœ… Better user experience

---

## Issue #3: Out of Memory with 15 Threads (Fixed v2.2, Dec 2025)

### Problem
Lambda crashed with `Runtime.OutOfMemory` error when using 15 parallel threads.

### Symptoms
```
REPORT: Max Memory Used: 6144 MB
Error Type: Runtime.OutOfMemory
```

### Root Cause
- Configuration: 15 threads, 25K batch size, 6GB Lambda memory
- Each thread processing 25K rows with 250 columns
- Peak memory per thread: ~450-500MB
- Total: 15 Ã— 500MB = 7.5GB > 6GB allocated

### Fix Applied
**File:** `config.json`

Reduced threads:
```json
{
  "parallel_threads": 10,  // Was 15
  "batch_size": 25000
}
```

Later increased Lambda memory to 10GB and set threads to 20.

### Impact
- âœ… Memory usage: ~5GB for 10 threads (safe within 6GB)
- âœ… No more OOM errors
- âœ… Later: 20 threads with 10GB for better throughput

### Lesson Learned
- Memory usage varies by chunk complexity
- Need 20-30% headroom for memory spikes
- Monitor "Max Memory Used" in CloudWatch REPORT lines

---

## Issue #4: Run Status Stuck in "running" (Fixed v2.1, Nov 2025)

### Problem
Even when all tables and chunks were marked as "completed", the migration run status remained "running" in the database.

### Root Cause
`migration_orchestrator.py` was missing code to update the run status in the database. It only returned the status in the API response but never persisted it to `migration_status.migration_runs`.

### Fix Applied
**File:** `scripts/migration_orchestrator.py` (line ~438)

Added status persistence at the end of `run_migration()`:
```python
# Update the run status in the database
if orchestrator.run_id:
    try:
        total_completed = sum(r.get('tables_completed', 0) for r in source_results.values())
        total_failed = sum(r.get('tables_failed', 0) for r in source_results.values())
        
        status_tracker.update_run_status(
            orchestrator.run_id,
            status=overall_status,
            completed_tables=total_completed,
            failed_tables=total_failed,
            total_rows_copied=total_rows_all_sources
        )
        logger.info(f"âœ“ Updated run status in database: {overall_status}")
    except Exception as e:
        logger.warning(f"Failed to update run status in database: {e}")
```

### Impact
- âœ… Run status now correctly updates to "completed" when done
- âœ… Resume logic no longer finds stale "running" runs
- âœ… Proper cleanup and reporting

---

## Issue #5: Missing ~50K Rows After "Completed" Migration (Fixed v2.0, 2025)

### Problem
- Snowflake source: 8,471,089 rows
- PostgreSQL target: 8,420,381 rows
- **Missing: ~50,708 rows (0.6%)**
- All chunks show status="completed"

### Root Cause: Sparse ID Distribution

The `NumericRangeStrategy` chunking creates chunks based on **ID range span**, not **actual data distribution**:

```python
# Problematic logic:
id_range = max_id - min_id + 1  # Total span of IDs
num_chunks = ceil(total_rows / batch_size)
chunk_step = ceil(id_range / num_chunks)  # Assumes uniform distribution!
```

#### Example Scenario:
```
Source Table IDs (Snowflake):
- IDs 1-1,000: 500 rows (0.05% of data) - VERY SPARSE
- IDs 1,001-1,000,000: 5,000 rows (0.59% of data) - SPARSE
- IDs 5,000,001-5,100,000: 8,465,589 rows (99.36% of data) - VERY DENSE!

Chunking Calculation:
- min_id = 1, max_id = 5,100,000
- id_range = 5,100,000
- total_rows = 8,471,089
- batch_size = 25,000
- num_chunks = ceil(8,471,089 / 25,000) = 339 chunks
- chunk_step = ceil(5,100,000 / 339) = 15,045 IDs per chunk

PROBLEM:
Most chunks process 0-100 rows (in sparse regions).
Dense regions get proper batching, BUT chunks can COMPLETELY MISS regions with data!
```

### Why Chunks Show "Completed" with 0 Rows
- A chunk querying `ID BETWEEN 1 AND 15,045` successfully runs
- Snowflake returns 0 rows (no error)
- COPY operation processes 0 rows â†’ Success!
- Status marked as "completed" âœ“
- **Nobody knows there was supposed to be data elsewhere**

### Real-World Triggers
1. **Gaps in ID sequence** (deleted records, ID jumps)
2. **Dense regions** outside the uniform distribution
3. **source_filter** that affects distribution
4. **Config changes** between runs (different filters, different batch sizes)

---

### Solutions

#### Solution 1: Use `source_filter` to Target Missing Data â­ **Recommended**

1. Find the current MAX(ID) in PostgreSQL:
```sql
SELECT MAX("ID") FROM conflict_dev.conflictvisitmaps;
-- Example result: 5,099,999
```

2. Update `config.json`:
```json
{
  "source": "CONFLICTVISITMAPS",
  "source_filter": "\"ID\" > 5099999",
  "truncate_onstart": false,
  "insert_only_mode": true
}
```

3. Run with `no_resume: true`

**Pros:**
- âœ… Fast - only processes missing data
- âœ… Safe - uses `insert_only_mode` to skip duplicates
- âœ… Targeted - explicitly handles the gap

#### Solution 2: Switch Chunking Strategy ðŸ”€ **Long-term**

If you have a date column (e.g., `CREATED_DATE`, `UPDATED_DATE`):

```json
{
  "source": "CONFLICTVISITMAPS",
  "chunking_columns": ["CREATED_DATE"],
  "chunking_column_types": ["timestamp"],
  "source_watermark": "UPDATED_DATE",
  "target_watermark": "updated_date"
}
```

**Pros:**
- âœ… Better for sparse IDs
- âœ… Natural incremental load support
- âœ… More predictable row distribution

---

### Key Learnings

1. **Chunking strategies matter**: Uniform range chunking assumes uniform data distribution
2. **Always validate row counts**: "Completed" status doesn't guarantee all data was processed
3. **Sparse IDs are dangerous**: Large ID gaps can cause chunks to miss dense regions
4. **Date-based chunking is safer**: Natural grouping, more predictable
5. **source_filter is powerful**: Use it to explicitly target data ranges for catch-up loads

---

## Troubleshooting Patterns

### Pattern: Migration Completes but Row Counts Don't Match

**Symptoms:**
- All chunks show "completed"
- No errors in logs
- Row count mismatch between source and target

**Check:**
1. Run row count comparison:
```sql
-- Source (Snowflake)
SELECT COUNT(*) FROM source_table;

-- Target (PostgreSQL)
SELECT COUNT(*) FROM target_table;
```

2. Check ID distribution:
```sql
-- Find gaps
WITH id_sequence AS (
    SELECT "ID", 
           "ID" - LAG("ID") OVER (ORDER BY "ID") as gap_size
    FROM source_table
)
SELECT COUNT(*) as gaps_found,
       SUM(gap_size) as total_gap_size
FROM id_sequence
WHERE gap_size > 1000;
```

3. Check chunking strategy in config.json

**Solutions:**
- Use date-based chunking for sparse IDs
- Use source_filter to target missing data
- Switch to OFFSET-based strategy (slow but complete)

---

### Pattern: Lambda Out of Memory

**Symptoms:**
```
Error Type: Runtime.OutOfMemory
Max Memory Used: [close to allocated memory]
```

**Check:**
1. CloudWatch "Max Memory Used" across multiple invocations
2. Current configuration:
   - `parallel_threads`
   - `batch_size`
   - Lambda memory allocation

**Solutions:**
1. Reduce `parallel_threads`
2. Reduce `batch_size`
3. Increase Lambda memory allocation
4. Memory calculation: threads Ã— ~500MB per thread

**Rule of Thumb:**
Keep memory usage at 70-80% of allocated memory for safety.

---

### Pattern: Duplicate Logs

**Symptoms:**
Every log entry appears multiple times in CloudWatch.

**Cause:**
Multiple logging handlers active (Lambda + application).

**Solution:**
Implement Lambda environment detection in logging setup (see Issue #1).

---

### Pattern: Slow Chunking

**Symptoms:**
- Migration stuck in "Creating chunks..." for minutes
- Many individual COUNT queries in Snowflake query history

**Cause:**
Individual queries for each date/range instead of aggregated query.

**Solution:**
Use aggregated query approach (see Issue #2).

---

## Verification Checklist

After any fix, verify:

```sql
-- 1. Check run status is correct
SELECT run_id, status, completed_at, total_rows_copied
FROM migration_status.migration_runs
ORDER BY started_at DESC LIMIT 5;

-- 2. Verify row counts match
SELECT 
    'source' as location,
    COUNT(*) FROM source_schema.table_name
UNION ALL
SELECT 
    'target' as location,
    COUNT(*) FROM target_schema.table_name;

-- 3. Check for any failed chunks
SELECT * FROM migration_status.migration_chunk_status
WHERE status = 'failed'
ORDER BY started_at DESC LIMIT 10;

-- 4. Check memory usage in CloudWatch
-- Look for "Max Memory Used" in REPORT lines
```

---

**Last Updated:** 2025-12-16  
**Note:** This document tracks resolved issues. For current issues, check TODO.md or PROJECT_CONTEXT.md

